{"cells":[{"cell_type":"markdown","id":"32e21f3e-bb9d-49c7-801f-8b86cb3f6c52","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network\" target=\"_blank\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"36af0f1c-617e-4d0f-aca7-46949a56fe1c","metadata":{},"outputs":[],"source":["# **Cracking the Code: Unlocking Success in Trading with Reinforcement Learning**\n"]},{"cell_type":"markdown","id":"cf0190c6-d8d2-4812-b839-f7131118788e","metadata":{},"outputs":[],"source":["Estimated time needed: **90** minutes\n"]},{"cell_type":"markdown","id":"d2831776-1831-463e-ae1a-851f8d682c37","metadata":{},"outputs":[],"source":["This project focuses on applying reinforcement learning techniques to develop an automated stock trading system. By leveraging technical indicators and a defined reward function, the project aims to train an intelligent agent capable of making informed decisions on buying, selling, or holding stocks. The project involves preprocessing stock data, defining the trading environment, training the reinforcement learning agent using algorithms like Deep Q-Network, and evaluating its performance on separate datasets. Through this project, the goal is to create a robust model that can adapt to changing market conditions and optimize trading strategies for potential profit.\n","\n","\u003e Disclaimer: This project is solely for learning purposes and does not serve as a financial trading advisor.\n","\n","| \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0SQEN/images/wance-paleri-HPM0CDfNtx0-unsplash.jpg\" width=\"400\" alt=\"trading; image credit: https://unsplash.com/\"\u003e |\n","|:--:| \n"]},{"cell_type":"markdown","id":"c258b984-8ec4-4cc6-bdd5-19b1b73454ad","metadata":{},"outputs":[],"source":["## __Table of Contents__\n","\n","\u003col\u003e\n","    \u003cli\u003e\u003ca href=\"#Objectives\"\u003eObjectives\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\n","        \u003ca href=\"#Setup\"\u003eSetup\u003c/a\u003e\n","        \u003col\u003e\n","            \u003cli\u003e\u003ca href=\"#Installing-Required-Libraries\"\u003eInstalling Required Libraries\u003c/a\u003e\u003c/li\u003e\n","            \u003cli\u003e\u003ca href=\"#Importing-Required-Libraries\"\u003eImporting Required Libraries\u003c/a\u003e\u003c/li\u003e\n","        \u003c/ol\u003e\n","    \u003c/li\u003e\n","    \u003cli\u003e\n","        \u003ca href=\"#Indicators\"\u003eTypes of Indicators\u003c/a\u003e\n","        \u003col\u003e\n","            \u003cli\u003e\u003ca href=\"#\"\u003eTrend Indicators\u003c/a\u003e\u003c/li\u003e\n","            \u003cli\u003e\u003ca href=\"#\"\u003eMomentum Indicators\u003c/a\u003e\u003c/li\u003e\n","            \u003cli\u003e\u003ca href=\"#\"\u003eVolume Indicators\u003c/a\u003e\u003c/li\u003e\n","            \u003cli\u003e\u003ca href=\"#\"\u003eVolatility Indicators\u003c/a\u003e\u003c/li\u003e\n","        \u003c/ol\u003e\n","    \u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#\"\u003eReinforcement Learning\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#\"\u003eDefining Environment\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#\"\u003eDefining Agent\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#\"\u003eTraining Agent\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#\"\u003eTesting Agent\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#\"\u003eTips on Tuning Agent\u003c/a\u003e\u003c/li\u003e\n","\u003c/ol\u003e\n","\n","\u003ca href=\"#Practice on more data\"\u003ePractice on More Data\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"26f3b19a-bcf2-4d9c-906a-b42380e52881","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"786ee434-a083-41ec-9480-6f75806bb004","metadata":{},"outputs":[],"source":["## Setup\n"]},{"cell_type":"markdown","id":"2a36ca18-637a-4c4b-a56a-4ad5ecc5f53b","metadata":{},"outputs":[],"source":["### Installing Required Libraries\n"]},{"cell_type":"markdown","id":"a7a0e5b2-9fd1-452f-89bd-6198a3c7d321","metadata":{},"outputs":[],"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"]},{"cell_type":"code","id":"c1249179-e091-49b9-81ba-9d97d0578ec2","metadata":{},"outputs":[],"source":["!pip install gym yfinance ta"]},{"cell_type":"markdown","id":"22b34f99-efd0-4d01-bf20-d471c172d95a","metadata":{},"outputs":[],"source":["Once you have installed that, please **restart your kernel**. You can do that by navigating to the button shown below:\n","\n","\u003ccenter\u003e \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/build-a-hotdog-not-hotdog-classifier-guided-project/images/Restarting_the_Kernel.png\" width=\"60%\" alt=\"Restart kernel\"\u003e \u003c/center\u003e\n","\n","\n","Now, let's proceed to check if this environment has a compatible GPU model or not:\n"]},{"cell_type":"markdown","id":"68a84260-99d3-44e5-b3ed-b5e1eb71dad3","metadata":{},"outputs":[],"source":["### Importing Required Libraries\n","\n","_We recommend you import all required libraries in one place (here):_\n"]},{"cell_type":"code","id":"cfc721a5-0c91-4f03-aedd-7be35c118c68","metadata":{},"outputs":[],"source":["# You can also use this section to suppress warnings generated by your code:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')\n\nimport gym\nimport yfinance as yf\nimport ta   # ta documentation: https://technical-analysis-library-in-python.readthedocs.io/en/latest/\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom itertools import accumulate\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nsns.set_style(\"darkgrid\")"]},{"cell_type":"markdown","id":"142348fb-f7db-4ba3-b47c-7c24a67df967","metadata":{},"outputs":[],"source":["--------\n"]},{"cell_type":"markdown","id":"088ed207-1010-4d36-9f50-df03e7babd80","metadata":{},"outputs":[],"source":["## Look a head:\n","We are aiming to implement reinforcement learning system for stock trading using indicators:\n","\n","Step 1: Data preparation\n","- Preprocess the stock dataset by cleaning and add indicators.\n","\n","Step 2: Define the environment \n","- Define the action space, which represents the actions the agent can take. In this case, the agent can either buy or sell the stock.\n","- Define the state space, which represents the current state of the stock. In this case, the state space can include the current price, the indicators.\n","- Define the reward function, which will be used to evaluate the agent's performance. In this case, the reward function can be based on the profit or loss made by the agent.\n","\n","Step 3: Define the agent\n","- Define the agent, which will take actions based on the current state of the stock and the reward function.\n","- Choose a reinforcement learning algorithm such as Q-Learning or Deep Q-Network (DQN) to train the agent.\n","\n","Step 4: Train the agent\n","- Train the agent using the stock data and the defined reward function.\n","- The agent should learn the optimal policy for buying and selling the stock based on the current state of the stock and the reward function.\n","\n","Step 5: Test the agent\n","- Test the agent on a separate dataset to evaluate its performance.\n","\n","Step 6: Refine the model\n","- Refine the model by adjusting the parameters of the reinforcement learning algorithm, the reward function, or the state space.\n","\n","------\n"]},{"cell_type":"markdown","id":"abccea1f-e560-4d20-9a7c-bc52f67e0e75","metadata":{},"outputs":[],"source":["## Loading the Data\n","\n"]},{"cell_type":"markdown","id":"32ec16ec-8d20-4d78-bbc8-098f2ebda869","metadata":{},"outputs":[],"source":["Using Yahoo Finance python library (`yfinance`), we are able to retrieve live data in each call.\n","\n","In the example below, we obtain hourly data of S\u0026P500 (the ticker is `^GSPC`) in the period of 2 years. You can change the ticker to `AAPL` to obtain Apple stock prices.\n"]},{"cell_type":"code","id":"85df62e8-338f-48a2-b614-7e972fb76fa1","metadata":{},"outputs":[],"source":["tickers = \"^GSPC\"  #\"^GSPC\" S\u0026P500\ndf = yf.download(tickers = tickers ,       # list of tickers\n                  period = \"2y\",         # time period\n                  interval = \"1h\",       # trading interval\n                  ignore_tz = True,      # ignore timezone when aligning data from different exchanges?\n                  prepost = False)       # download pre/post market hours data?  \nprint(\"shape of dataset: \",df.shape)\ndf.head()"]},{"cell_type":"markdown","id":"15fd5bbd-3cef-4372-80e4-55941524293c","metadata":{},"outputs":[],"source":["You can see that the `yfinance` library returns a dataframe with 6 columns. To plot the price, we have selected the `Adj Close` column, which represents the adjusted price taking into account any stock splits that may have occurred prior to the specified date. We specify the `split_line` which is the point of separating data into Training and Testing.\n"]},{"cell_type":"code","id":"df6c203f-4a02-4bd5-8d64-55780dabe8dd","metadata":{},"outputs":[],"source":["split_line = int(np.round(len(df)* 0.1)) # use 10% of data for testing"]},{"cell_type":"code","id":"95a973d7-1c99-4306-b856-b1674abaca45","metadata":{},"outputs":[],"source":["# Get min and max of 'Adj Close'\nadj_close_min = df['Adj Close'].min()\nadj_close_max = df['Adj Close'].max()\n\n# Set the background color\nsns.set_style(rc = {'axes.facecolor': 'lightsteelblue'})\nplt.figure(figsize=(12,4))\n\n# Create line plot\nsns.lineplot(data=df[\"Adj Close\"],label = \"Adjusted Close Price\")\nplt.xticks(rotation=45)\nplt.title(tickers)\n\n# single vline with full ymin and ymax\nplt.vlines(x=df.index[-split_line], ymin=adj_close_min, ymax=adj_close_max, colors='green', ls=':', lw=2, label='Train-Test Split Point')\n\nplt.legend(bbox_to_anchor=(1.0, 1), loc='upper left')\nplt.show()\n"]},{"cell_type":"markdown","id":"22facd41-22cf-41e2-af18-87914172415e","metadata":{},"outputs":[],"source":["## Step 1: Data Preparation (Adding Trading Indicators)\n"]},{"cell_type":"markdown","id":"521886fa-fd70-4a3a-8774-0c8f1503f6c8","metadata":{},"outputs":[],"source":["Trading indicators are tools used by traders to analyze market data and make informed trading decisions. There are several types of trading indicators, each with unique characteristics and applications. Here are five types of trading indicators with an example for each:\n","  \n","\n","1. Momentum Indicators\n","2. Volume Indicators\n","3. Volatility Indicators\n","4. Trend Indicators\n","5. Support and Resistance\n","\n","\n","Remember that no single indicator is perfect, and traders often use a combination of indicators to make more informed decisions. It's essential to understand the strengths and limitations of each indicator and apply them in the context of your trading strategy and risk management.\n","\n","\u003e ___The Good trading strategy is the one that employs all types of market behaviour (all types of indicator)___\n"]},{"cell_type":"markdown","id":"7292eb11-cfc2-4882-99ef-dc44d6766ec2","metadata":{},"outputs":[],"source":["### Trend Indicators \n","\n","These indicators help traders identify the direction and strength of a market trend. They typically measure the asset's price movement over time and help traders determine if a trend is bullish (upward) or bearish (downward).\n","\n","Example: Moving Averages (MA)\n","Moving averages smooth out price data by calculating the average price of an asset over a specified time period. Popular types of moving averages include the Simple Moving Average (SMA) and the Exponential Moving Average (EMA). Traders often use moving averages to identify trends and potential entry/exit points.\n"]},{"cell_type":"markdown","id":"c68b4e63-aa2f-4360-8aa3-43bbfbf725c2","metadata":{},"outputs":[],"source":["#### EMAs : Exponential moving averages\n"]},{"cell_type":"code","id":"cf60c136-39ed-4d8d-a919-a1b738b6857b","metadata":{},"outputs":[],"source":["df['EMA7'] = ta.trend.EMAIndicator(close= df['Adj Close'], window= 7, fillna= False).ema_indicator()\ndf['EMA14'] = ta.trend.EMAIndicator(close= df['Adj Close'], window= 14, fillna= False).ema_indicator()\ndf['EMA50'] = ta.trend.EMAIndicator(close= df['Adj Close'], window= 50, fillna= False).ema_indicator()\ndf['EMA200'] = ta.trend.EMAIndicator(close= df['Adj Close'], window= 50, fillna= False).ema_indicator()"]},{"cell_type":"markdown","id":"98ce9a4d-d6b4-47c1-9c05-a2607d0c9b62","metadata":{},"outputs":[],"source":["Plotting the EMAs.\n"]},{"cell_type":"code","id":"92f9d42d-8f41-4631-a1a5-deb44e35a7a7","metadata":{},"outputs":[],"source":["# Set the background color\nsns.set_style(rc = {'axes.facecolor': 'lightsteelblue'})\nplt.figure(figsize=(12,4))\n\n# Create line plot\nsns.lineplot(data=df[\"Adj Close\"],label = \"Adjusted Close Price\")\nsns.lineplot(data=df[\"EMA7\"],label = \"EMA7\")\nsns.lineplot(data=df[\"EMA14\"],label = \"EMA14\")\nsns.lineplot(data=df[\"EMA50\"],label = \"EMA50\")\nsns.lineplot(data=df[\"EMA200\"],label = \"EMA200\")\n\n\nplt.xticks(rotation=45)\nplt.title(tickers)"]},{"cell_type":"markdown","id":"1818b38c-f2c1-48d5-9beb-f90654cd4b22","metadata":{},"outputs":[],"source":["### Momentum Indicators \n","\n","These indicators measure the strength and rate of change in price movements. They help traders identify potential turning points in the market and can indicate whether an asset is overbought or oversold.\n","\n","Example: Relative Strength Index (RSI) and MACD\n","The RSI is a popular momentum oscillator that measures the speed and change of price movements on a scale of 0-100. An RSI reading below 30 typically indicates that an asset is oversold, while a reading above 70 suggests that it is overbought.\n","\n","The Moving Average Convergence Divergence (MACD) is a momentum indicator. It helps traders identify trend direction, potential trend reversals, and the strength of price movements. The MACD is calculated by subtracting a longer-term Exponential Moving Average (EMA) from a shorter-term EMA, usually the 26-day and 12-day EMAs, respectively. A signal line, typically the 9-day EMA of the MACD, is also plotted on the same chart.\n"]},{"cell_type":"markdown","id":"781020cb-2039-4c95-9243-0080d3cc751d","metadata":{},"outputs":[],"source":["\n"," \n"," | \u003cimg src=\"https://commodity.com/wp-content/uploads/technical-analysis/MACDbasicsQQQQ.gif\" width=\"500\" alt=\"MACD\"\u003e|\n","|:--:| \n","| *An example of MACD, image credit: commodity.com*  |\n"]},{"cell_type":"code","id":"c3041510-0e24-4375-b077-2abf9096d5d9","metadata":{},"outputs":[],"source":["df['MACD_line'] =ta.trend.MACD(close= df['Adj Close'], window_slow= 26, window_fast= 12, window_sign = 9, fillna = False).macd()\ndf['MACD_signal'] =ta.trend.MACD(close= df['Adj Close'], window_slow= 26, window_fast= 12, window_sign = 9, fillna = False).macd_signal()\ndf['MACD_diff'] =ta.trend.MACD(close= df['Adj Close'], window_slow= 26, window_fast= 12, window_sign = 9, fillna = False).macd_diff()"]},{"cell_type":"markdown","id":"06bb7046-3b18-47ff-a2c2-ef2ef8531e75","metadata":{},"outputs":[],"source":["#### RSI: Relative Strength Index \n"]},{"cell_type":"markdown","id":"8b691b3d-7796-4d32-9675-3976ebb6800d","metadata":{},"outputs":[],"source":["\n","\n","| \u003cimg src=\"https://www.investopedia.com/thmb/lXq2-HwfRcFpe_0NsEieuTXXaUw=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/dotdash_final_Relative_Strength_Index_RSI_Jul_2020-01-98fcd0c032cb4f0da005c31c44440c90.jpg\" width=\"500\" alt=\"RSI, credit: www.investopedia.com\"\u003e|\n","|:--:| \n","| An example of RSI indicator *image credit: https://pixabay.com/* |\n"]},{"cell_type":"code","id":"f3f3d612-ca45-4fc6-88b9-ded350639a05","metadata":{},"outputs":[],"source":["df['RSI']=ta.momentum.RSIIndicator(close= df['Adj Close'], window= 14, fillna= False).rsi()"]},{"cell_type":"markdown","id":"180ed530-19f4-4906-bf11-6676f93005d1","metadata":{},"outputs":[],"source":["### Volume Indicators \n","\n","\u003e These indicators analyze the trading volume to assess the strength and significance of price movements. High volume can confirm the validity of a trend or signal a reversal.\n","\n","\u003e On-Balance Volume (OBV) is a cumulative indicator that adds volume on up days and subtracts volume on down days. It helps traders identify buying or selling pressure in the market. A rising OBV indicates strong buying pressure, while a falling OBV signals strong selling pressure.\n","4. Volatility Indicators: These indicators measure the degree of price fluctuations and help traders gauge market sentiment and potential price breakouts.\n"]},{"cell_type":"code","id":"284acac4-e71d-496d-a7d6-15f1dfc62e32","metadata":{},"outputs":[],"source":["# adding Volumn indicator\ndf['OBV']= ta.volume.OnBalanceVolumeIndicator(close= df['Adj Close'], volume= df['Volume'], fillna = False).on_balance_volume()\ndf.head()"]},{"cell_type":"markdown","id":"792b8c76-1f79-41bc-a2b7-a115dcef4631","metadata":{},"outputs":[],"source":["### Volatility Indicators\n","\u003e These indicators measure the degree of price fluctuations and help traders gauge market sentiment and potential price breakouts.\n","\n","\u003e Bollinger Bands consist of a centerline (typically an SMA) and two outer bands, which are standard deviations above and below the centerline. The bands expand and contract based on market volatility. When the bands are narrow, volatility is low, and when they widen, volatility is high. Traders use Bollinger Bands to identify potential entry/exit points and overbought/oversold conditions.\n","\n","\n","| \u003cimg src=\"https://a.c-dn.net/b/3u3xku/trading-forex-with-bollinger-bands_body_Mainimage.png\" width=\"50%\" alt=\"BB image credit:https://www.dailyfx.com/\"\u003e|\n","|:--:| \n","| An example of MACD indicator *image credit: https://www.dailyfx.com/* |\n"]},{"cell_type":"code","id":"d65921af-a861-4313-8586-b754d2dd3089","metadata":{},"outputs":[],"source":["df['BBH']=ta.volatility.BollingerBands(df['Adj Close'], window = 20, window_dev = 2, fillna = False).bollinger_hband_indicator()\ndf['BBL']=ta.volatility.BollingerBands(df['Adj Close'], window = 20, window_dev = 2, fillna = False).bollinger_lband_indicator()\ndf.head()"]},{"cell_type":"code","id":"1c0e678f-7ce6-4ead-9c72-0f6cbc72910f","metadata":{},"outputs":[],"source":["df.shape"]},{"cell_type":"code","id":"c8761dbc-accf-43ef-8875-17bda90c0a86","metadata":{},"outputs":[],"source":["df.columns"]},{"cell_type":"code","id":"710e5fd4-1aeb-4ae8-8a63-69488337375e","metadata":{},"outputs":[],"source":["df=df.dropna()\ndf = df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'],axis=1)\ndf.head()"]},{"cell_type":"code","id":"7b9e24a8-76b3-493e-a58e-0b1c10c10093","metadata":{},"outputs":[],"source":["train_data = df.iloc[:-split_line,:]\ntest_data = df.iloc[-split_line:,:]\ninput_shape= train_data.shape[1]\nprint(input_shape)\nprint('Training data:',train_data.shape)\nprint('Testing data',test_data.shape)"]},{"cell_type":"markdown","id":"d97384fa-1f80-4a1e-b20f-b5732b7c1d6c","metadata":{},"outputs":[],"source":["## Step 2: Define the environment\n"]},{"cell_type":"markdown","id":"506b925a-733a-4621-9c28-7cd5b0e1f199","metadata":{},"outputs":[],"source":["\n","\n","|  |  |\n","|-----|---|\n","|    | **Environment**  |\n","| \u003cimg style=\"float: left;\" src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0SQEN/images/_2800865b-9afe-411a-9d9c-80e322f466b3.jpeg\" width=\"900\" alt=\"environment\"\u003e | \u003cspan style=\"font-family: Verdana, sans-serif; font-size: 15px;\"\u003e The `StockTradingEnv` is a custom environment for stock trading, which extends the `gym.Env` class from OpenAI's Gym library. The environment is initialized with stock data containing daily stock prices and various technical indicators. The environment's purpose is to simulate a stock trading scenario where an agent can decide to buy or sell shares based on the provided market data.\u003c/span\u003e \u003cbr\u003e  \u003cbr\u003e The main components of the environment are: \u003cbr\u003e - Initialization \u003cbr\u003e - Reset \u003cbr\u003e - Step \u003cbr\u003e - Get observation |\n","\n","\n","\n","\n","\n","1. **Initialization** : The environment is initialized with stock data and defines the action and observation spaces. The action space is discrete, with two possible actions (0: Buy, 1: Sell). The observation space is continuous, consisting of the stock's closing price and the values of various technical indicators at the current time step. \n","    - In the  code, `self.action_space = gym.spaces.MultiDiscrete([3, 10])` defines the action space for the StockTradingEnv environment. `gym.spaces.MultiDiscrete` is a class from the OpenAI Gym library that is used to define a multidimensional discrete action space. It accepts a list of integers, where each integer specifies the number of possible values for a particular dimension. In this case, [3, 10] is passed to the MultiDiscrete class, creating a 2D action space with the following properties: The first dimension has 3 possible values (0, 1, 2), which represent different action types: buy (0), sell (1), or hold (2). The second dimension has 10 possible values (0, 1, 2, ..., 9), representing the number of shares to trade during the buy or sell action.\n","    \n","2. **Reset** : The `reset` method resets the environment to its initial state, setting the current step to 0, resetting profits, shares held, and net worth, and returning the initial observation. \n","3. **Step** : The `step` method takes in an action and updates the environment state accordingly. If the action is to buy, the environment calculates the number of shares that can be bought with the available cash, deducts the purchase price from the cash balance, and adds the purchased shares to the shares held variable. If the action is to sell, the environment sells the specified number of shares, adds the sale price to the cash balance, and calculates the profits made based on the difference between the purchase and sale prices. The method returns the updated observation, profits, done flag, and an empty dictionary.\n","4. **Get observation** : The `_get_obs` method returns the current observation, which is a numpy array containing the stock's closing price and the values of various technical indicators at the current time step.\n","\n","The environment provides a framework for training reinforcement learning agents in a stock trading scenario by allowing them to take actions (buy or sell shares) based on the available market data and learn from the resulting profits.\n","\n","___Exercise: Fill the gap with the proper code___\n"]},{"cell_type":"code","id":"53e0fcf1-8534-4ffe-b0ea-ddd32371556e","metadata":{},"outputs":[],"source":["import gym\nimport numpy as np\n\nclass StockTradingEnv(gym.Env):\n    def __init__(self, data, initial_cash=10000, commission=0.000):\n        # Define the action space as a 2D vector representing the action type (buy or sell) and number of shares to trade\n        self.action_space = gym.spaces.MultiDiscrete([3, 10])  # [action, number of shares]\n\n        # Define the observation space as a X-D vector containing various indicators of the stock price\n        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(12,), dtype=np.float32)\n\n        # Store the historical stock data, initial cash balance, and commission rate as instance variables\n        self.data = data\n        self.initial_cash = initial_cash\n        self.commission = commission\n        self.end_step = len(self.data) - 1\n\n        # Reset the environment to its initial state\n        self.reset()\n\n    def reset(self):\n        # Set the current step to 0 and reset the state variables\n        self.current_step = 0\n        self.profits = 0\n        self.shares_held = 0\n        self.cash = self.initial_cash\n        self.buy_price = 0\n        self.sell_price = 0\n        self.done = False\n\n        # Return the initial observation\n        return self._get_obs()\n\n    def step(self, action):\n        action_type, amount = action\n\n        if action_type == 0:\n            cost = self.data['Adj Close'][self.current_step] * amount\n            if self.cash \u003e= cost:\n                self.cash -= cost\n                self.shares_held += amount\n                self.buy_price = self.data['Adj Close'][self.current_step]\n        elif action_type == 1:\n            shares_sold = min(self.shares_held, amount)\n            revenue = self.data['Adj Close'][self.current_step] * shares_sold\n            self.cash += revenue\n            self.profits += (self.data['Adj Close'][self.current_step] - self.buy_price) * shares_sold\n            self.shares_held -= shares_sold\n            self.sell_price = self.data['Adj Close'][self.current_step]\n\n        self.current_step += 1\n\n        # Calculate the total portfolio value\n        portfolio_value = self.cash + (self.shares_held * self.data['Adj Close'][self.current_step])\n  \n        # Calculate the reward based on the change in portfolio value at each stept\n        reward = portfolio_value - (self.cash + (self.shares_held * self.data['Adj Close'][self.current_step - 1]))\n        done = (self.current_step == self.end_step)\n        return self._get_obs(), reward, done, {}\n\n\n    def _get_obs(self):\n    # Calculate the observation vector for the current step\n        obs = np.array([\n            self.data['Adj Close'][self.current_step],\n            # try yourself\n            # get the current observation for each indicator\n            # write your code here\n            \n            \n            \n            \n            \n        ])\n\n        # Return the observation vector\n        return obs\n"]},{"cell_type":"markdown","id":"7f3fe4f3-105c-4bce-905c-f8632c6cff4d","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\n","    \u003csummary\u003eClick here for Solution\u003c/summary\u003e\n","    \n","```python\n","    def _get_obs(self):\n","    # Calculate the observation vector for the current step\n","        obs = np.array([\n","            self.data['Adj Close'][self.current_step],\n","            self.data['EMA7'][self.current_step],\n","            self.data['EMA14'][self.current_step],\n","            self.data['EMA50'][self.current_step],\n","            self.data['EMA200'][self.current_step],\n","            self.data['MACD_line'][self.current_step],\n","            self.data['MACD_signal'][self.current_step],\n","            self.data['MACD_diff'][self.current_step],\n","            self.data['RSI'][self.current_step],\n","            self.data['OBV'][self.current_step],\n","            self.data['BBH'][self.current_step],\n","            self.data['BBL'][self.current_step],\n","        ])\n","```\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"ff63c074-2528-42a1-b3b7-fc2c5fbbc5fc","metadata":{},"outputs":[],"source":["\n","___We defines a custom OpenAI gym environment for simulating a simple stock trading scenario, and implements the necessary methods for the reinforcement learning agent to interact with the environment. The observation and action spaces, as well as the reward function, can be customized to suit different trading strategies and datasets.___\n"]},{"cell_type":"markdown","id":"0fd7676b-78f5-497a-80cf-30564f8bd4e1","metadata":{},"outputs":[],"source":["## Step 3: Define The Agent\n","| \u003cimg src=\"https://cdn.pixabay.com/photo/2019/01/22/10/58/pixel-cells-3947912_1280.png\" width=\"200\" alt=\"Cyber attack image\"\u003e |\n","|:--:| \n","| *image credit: https://pixabay.com/* |\n","\n","\n","\n","The `DQNAgent` class defines a deep Q-learning agent that uses a neural network to approximate the Q-function. This agent learns to make decisions in the stock trading environment based on the market data provided in the environment's observations.\n","\n","The agent's main components are described as follows:\n","\n","\n","\u003cdiv style=\"display: flex; align-items: center;\"\u003e\n","  \n","\n","\n","\n","1. **Initialization**: The agent is initialized with the state size, action space, and various learning parameters, such as gamma, epsilon, epsilon decay, and learning rate. The agent also initializes a neural network model using the `_build_model` method.\n","2. **Build Model**: The `_build_model` method defines a neural network architecture using TensorFlow's Keras API. The network consists of two hidden layers with 64 neurons each and ReLU activation functions. The output layer has a linear activation function, and the number of output neurons is equal to the size of the action space. The model is compiled using the mean squared error loss function and the Adam optimizer.\n","3. **Remember**: The `remember` method stores the agent's experiences (state, action, reward, next state, done) in a memory buffer, implemented as a deque with a maximum length of 2000. This buffer is used to sample experiences for training the agent's neural network.\n","4. **Act**: The `act` method selects an action based on the agent's current state. With probability epsilon, the agent selects a random action, and with probability 1 - epsilon, the agent selects the action with the highest Q-value predicted by the neural network. The agent's epsilon value decays over time to balance exploration and exploitation.\n","5. **Replay**: The `replay` method samples a batch of experiences from the memory buffer and uses them to train the agent's neural network. The agent computes target Q-values for the selected actions based on the rewards and discounted future Q-values of the next states. The agent updates the neural network weights by minimizing the mean squared error between the predicted Q-values and the target Q-values.\n","\n","\n","\u003c/div\u003e\n","\n","\u003cbr\u003e\n","\n","The `DQNAgent` class provides a framework for training a deep reinforcement learning agent to make buy and sell decisions in the stock trading environment using deep Q-learning. The agent learns to approximate the Q-function through a neural network and balances exploration and exploitation using an epsilon-greedy action selection strategy.\n"]},{"cell_type":"code","id":"25af5965-ae65-43b8-a936-d731ef2ec10f","metadata":{},"outputs":[],"source":["import random\nimport numpy as np\nfrom collections import deque\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD, Adam\n\nclass DQNAgent:\n    def __init__(self, state_size, action_space,memory_size=3000):\n        # Initialize instance variables\n        self.state_size = state_size\n        self.action_space = action_space\n        self.action_size = action_space.nvec.prod()\n        self.memory = deque(maxlen=memory_size)\n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.01\n        self.model = self._build_model()\n\n    def _build_model(self):\n        # Build the neural network model\n        model = Sequential()\n        model.add(Dense(64, input_dim=self.state_size))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dense(256, activation='relu'))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dense(self.action_size, activation='relu'))\n        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n        return model\n\n    def remember(self, state, action, reward, next_state, done):\n        # Store the experience in memory\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        # Choose an action using the epsilon-greedy strategy\n        if np.random.rand() \u003c= self.epsilon:\n            return (random.randrange(self.action_space.nvec[0]), random.randrange(self.action_space.nvec[1]))\n        act_values = self.model.predict(state)\n        return np.unravel_index(np.argmax(act_values[0]), self.action_space.nvec)\n\n    def replay(self, batch_size):\n        # Train the model using random samples from memory\n        minibatch = random.sample(self.memory, batch_size)\n        for state, action, reward, next_state, done in minibatch:\n            target = reward\n            if not done:\n                # Update target value for non-terminal states\n                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n            target_f = self.model.predict(state)\n\n            # Convert the action tuple to a linear index\n            action_idx = np.ravel_multi_index(action, self.action_space.nvec)\n\n            # Update the target value for the chosen action\n            target_f[0][action_idx] = target\n            self.model.fit(state, target_f, epochs=1, verbose=0)\n\n        # Decay the exploration rate if it's above the minimum threshold\n        if self.epsilon \u003e self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n"]},{"cell_type":"markdown","id":"f269bb52-4419-481c-abd6-39588eb5c8c7","metadata":{},"outputs":[],"source":["## Step 4: Train the Agent\n","\n","In this step, we train a `DQNAgent` in the `StockTradingEnv` using a deep Q-learning algorithm. The training process consists of the following steps: \n","\n","\n","\u003cdiv style=\"display: flex; align-items: center;\"\u003e\n","  \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0SQEN/_84d78f91-ffa6-43b7-ac39-f3b229beac78.jpeg\" width=\"30%\" alt=\"Image\" style=\"padding-right: 10px;\"\u003e\n","\n","\n","\n","1. Load the stock training data and create a StockTradingEnv using the data. Define the state size and action size based on the environment's observation space and action space. \n","2. Create a DQNAgent using the state size and action space from the environment. Set the number of training episodes and the batch size for replaying experiences. \n","3. Loop through the episodes and reset the environment at the beginning of each episode. Reshape the initial state to match the input format expected by the agent's neural network. Initialize a variable to keep track of the total reward for the episode. \n","4. Within each episode, the agent selects an action based on its current state and the epsilon-greedy exploration strategy. The agent then performs the action in the environment and receives the next state, reward, and a flag indicating whether the episode has ended. \n","5. The agent stores the experience (state, action, reward, next_state, done) in its memory buffer and updates its current state to the next state. \n","6. At the end of each episode, print the episode number, total reward, and current epsilon value. If the agent's memory buffer has enough experiences, perform a replay step to update the neural network weights using a batch of sampled experiences.\n","\n","    \n","\u003c/div\u003e\n","\u003cbr\u003e\n","\n","The training process allows the agent to learn a policy for buying and selling stocks by interacting with the environment, updating its neural network, and balancing exploration and exploitation with the epsilon-greedy action selection strategy.\n"]},{"cell_type":"code","id":"3451ced6-cc91-4821-818b-78da7899c1ed","metadata":{},"outputs":[],"source":["# Load the stock data\ndata = train_data\n\n# Create the environment\nenv = StockTradingEnv(data)\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.nvec.prod()\n\n# Create and train the agent\nagent = DQNAgent(state_size, env.action_space)\nepisodes = 100\nbatch_size = 32\n\nfor e in tqdm(range(episodes)):\n    state = env.reset()\n    state = np.reshape(state, [1, state_size])\n    total_reward = 0\n\n    while True:\n        action = agent.act(state)\n        next_state, reward, done, _ = env.step(action)\n        total_reward += reward\n        next_state = np.reshape(next_state, [1, state_size])\n\n        agent.remember(state, action, reward, next_state, done)\n        state = next_state\n\n        if done:\n            # print(f'Episode: {e + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2}')\n            break\n\n    if len(agent.memory) \u003e batch_size:\n        agent.replay(batch_size)\n"]},{"cell_type":"markdown","id":"3703ee4a-0484-4ab1-bfe6-243289425ccf","metadata":{},"outputs":[],"source":["## Step 5: Test the agent\n","\n","Test the agent on a separate dataset to evaluate its performance. To test the agent on a separate dataset, we can create a new instance of the StockTradingEnv with the new dataset and use the trained agent to make decisions.\n"]},{"cell_type":"code","id":"07d76ef7-cbe6-4b6a-85c7-14180fa8939e","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n\n# Load the test data\ntest_data = test_data.iloc[:,:]\n\n# change this to 1 to get records of all transactions\nverbose = 0\n\n# Create the test environment\ntest_env = StockTradingEnv(test_data, initial_cash=10000)\n\n# Test the agent\nstate = test_env.reset()\nstate = np.reshape(state, [1, state_size])\ntotal_reward = 0\n\n# Initialize lists to store the buy and sell prices and the corresponding time steps\nbuy_prices = []\nbuy_times = []\nsell_prices = []\nsell_times = []\n\n# Initialize the list to store portfolio values and add the initial portfolio value\nportfolio_values = [test_env.cash]\n\nwhile True:\n    action = agent.act(state)\n    next_state, reward, done, _ = test_env.step(action)\n\n    # Calculate the total portfolio value\n    portfolio_value = test_env.cash + (test_env.shares_held * test_env.data['Adj Close'][test_env.current_step])\n    portfolio_values.append(portfolio_value)\n\n    # Print the action details\n    action_type, shares = action\n    if(verbose ==1):\n        if action_type == 0:\n            print(f\"Buy {shares} shares at price {test_env.data['Adj Close'][test_env.current_step]}\")\n        elif action_type == 1:\n            print(f\"Sell {shares} shares at price {test_env.data['Adj Close'][test_env.current_step]}\")\n\n        # Print available cash, number of shares held, and reward\n        print(f\"Available cash: {test_env.cash}\",f\",Shares held: {test_env.shares_held}\",f\",Reward: {reward}\")\n        print(\"------------------------------------------------\")\n\n    total_reward += reward\n    next_state = np.reshape(next_state, [1, state_size])\n    state = next_state\n    \n    # Check if a buy action was taken and record the price and time step\n    if action[0] == 0:\n        buy_prices.append(test_data['Adj Close'][test_env.current_step])\n        buy_times.append(test_data.index[test_env.current_step])\n\n    # Check if a sell action was taken and record the price and time step\n    elif action[0] == 1:\n        sell_prices.append(test_data['Adj Close'][test_env.current_step])\n        sell_times.append(test_data.index[test_env.current_step])\n\n    if done:\n        print(f'Test: Total Reward: {total_reward}')\n        break\n\n# Calculate the percentage change in the portfolio value\nportfolio_percentage_changes = [((value - portfolio_values[0]) / portfolio_values[0]) * 100 for value in portfolio_values]\n\n# Create the figure with two subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n\n# Plot the historical price data as a line chart on the first subplot\nax1.plot(test_data.index, test_data['Adj Close'])\n\n# Plot the buy and sell actions as red and blue dots, respectively, on the first subplot\nax1.scatter(buy_times, buy_prices, color='b', marker='o', label='Buy')\nax1.scatter(sell_times, sell_prices, color='r', marker='o', label='Sell')\n\n# Set the first subplot's title, x-label, and y-label\nax1.set_title('Trading History')\nax1.set_xlabel('Time')\nax1.set_ylabel('Price')\nax1.legend()\n\n# Plot the percentage change in the portfolio value on the second subplot\nax2.plot(test_data.index, portfolio_percentage_changes, color='g', linestyle='--', label='Portfolio % Change')\n\n# Set the second subplot's title, x-label, and y-label\nax2.set_title('Portfolio Value Change')\nax2.set_xlabel('Time')\nax2.set_ylabel('% Change')\nax2.legend()\n\n# Display the chart\nplt.show()\n"]},{"cell_type":"markdown","id":"1febc2c7-4ea7-48a4-bf71-0fc10f3701ce","metadata":{},"outputs":[],"source":["___Run the test again and you will get different result.___\n","\n","\n","\u003ccenter\u003e \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0SQEN/images/meme_honest_work.jpeg\" width=\"40%\" alt=\"Restart kernel\"\u003e \u003c/center\u003e\n","\n","\n","When testing the DQN agent, you may observe different results each time due to the agent's exploration strategy, which is governed by the epsilon-greedy policy. The agent uses an exploration rate (`epsilon`) to balance between exploration (trying new actions) and exploitation (using the current best-known action). The agent selects random actions with probability `epsilon`, and as long as `epsilon` is greater than zero, there will be some degree of randomness in the agent's decisions.\n","\n","If you want your agent to act deterministically during testing, you can set the `epsilon` value to zero, which forces the agent to always choose the best-known action based on its current knowledge without any exploration. To do this, you can add a line of code to set `epsilon` to zero before testing:\n","\n","```python\n","\n","agent.epsilon = 0\n","```\n","\n","By doing this, your agent will act deterministically, and you should get the same results each time you test it. Keep in mind that doing this will prevent the agent from exploring new actions, which might be suboptimal if the agent hasn't fully learned the optimal policy during training.\n"]},{"cell_type":"markdown","id":"7120b026-fe23-4a22-90ec-f60eb6fe4766","metadata":{},"outputs":[],"source":["### Monte Carlo for Testing\n","\n","\n","| \u003cimg src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Pi_30K.gif/440px-Pi_30K.gif\" width=\"300\" alt=\"human analysis\"\u003e |\n","|:--:| \n","| *Monte Carlo method applied to approximating the value of π.* [Wikipedia](https://en.wikipedia.org/wiki/Monte_Carlo_method#:~:text=Monte%20Carlo%20methods%2C%20or%20Monte,might%20be%20deterministic%20in%20principle.) |\n","\n","\n","\n","For better testing we utilize Monte Carlo method. the concept of Monte Carlo refers to running multiple simulations or tests of the trained agent to estimate the expected performance of the agent. Each run or simulation uses the same agent but with different starting conditions, which are determined by the initial state provided by the environment. This is particularly useful when dealing with environments that have some level of randomness or stochasticity, as it helps provide a more accurate and robust estimate of the agent's performance.\n","\n","By running multiple tests (Monte Carlo runs), we are essentially exploring various possible scenarios in the stock trading environment. The agent's decisions and the subsequent rewards obtained can vary across different runs due to differences in the initial state or changes in the agent's exploration-exploitation trade-off as a result of its epsilon-greedy strategy.\n","\n","After all the Monte Carlo runs are completed, we calculate the average reward over all runs. This average reward gives us a more reliable estimate of the agent's performance than the reward from a single test run, as it captures the agent's behavior across various scenarios. The higher the number of Monte Carlo runs, the more accurate and reliable the estimate of the agent's performance becomes, as it converges to the true expected value.\n"]},{"cell_type":"code","id":"a884e180-a4b2-496d-a7c0-535fa9505b11","metadata":{},"outputs":[],"source":["# Load the test data\ntest_data = test_data\n\n# Create the test environment\ntest_env = StockTradingEnv(test_data,initial_cash =100000)\n\n# Define the number of Monte Carlo runs\nmonte_carlo_runs = 200\nrewards = []\n\nfor run in tqdm(range(monte_carlo_runs)):\n    # Test the agent\n    state = test_env.reset()\n    state = np.reshape(state, [1, state_size])\n    total_reward = 0\n\n    while True:\n        action = agent.act(state)\n        next_state, reward, done, _ = test_env.step(action)\n        total_reward += reward\n        next_state = np.reshape(next_state, [1, state_size])\n        state = next_state\n\n        if done:\n            # print(f'Test Run: {run + 1}/{monte_carlo_runs}, Total Reward: {total_reward}')\n            rewards.append(total_reward)\n            break\n\n# Calculate and print the average reward\naverage_reward = np.mean(rewards)\nprint(f'Average Reward after {monte_carlo_runs} runs: {average_reward}')\n"]},{"cell_type":"markdown","id":"58d7f61a-dffa-4d99-ada4-c0d4f94e6879","metadata":{},"outputs":[],"source":["\u003ccenter\u003e \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0SQEN/images/meme_not_bad2.jpeg\" width=\"40%\" alt=\"Obama\"\u003e \u003c/center\u003e\n"]},{"cell_type":"markdown","id":"1c63dc3a-f76e-42ae-89ae-f4b703a33779","metadata":{},"outputs":[],"source":["## Step 6: Refine the model\n","\n","\n","Refining a reinforcement learning model involves adjusting its parameters to improve its performance. Here are some common ways to refine a model: \n","\n","\n","\u003cdiv style=\"display: flex; align-items: center;\"\u003e\n","  \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0SQEN/images/_5b34b14b-bc6f-4955-b81b-6014f3d08950.jpeg\" width=\"30%\" alt=\"Image\" style=\"padding-right: 10px;\"\u003e\n","\n","    \n","1. Adjust the hyperparameters of the reinforcement learning algorithm: Most reinforcement learning algorithms have several hyperparameters, such as the learning rate, discount factor, and exploration rate, that can be adjusted to improve performance. Experiment with different hyperparameters to find the optimal values. \n","2. Adjust the reward function: The reward function determines the agent's objective, so it can have a significant impact on its behavior. Consider adjusting the reward function to better incentivize the desired behavior. \n","3. Adjust the state space: The state space determines the information that the agent has access to when making decisions. Adding or removing features from the state space can improve the agent's ability to make accurate predictions. \n","4. Collect more training data: Training a reinforcement learning model can be data-intensive, so collecting more training data can improve its performance. Consider increasing the amount of training data or collecting data from a wider range of scenarios. \n","5. Change the architecture of the neural network: Many reinforcement learning algorithms use neural networks to estimate values or policies. Changing the architecture of the neural network can sometimes improve performance. \n","6. Use a different reinforcement learning algorithm: There are many different reinforcement learning algorithms, each with its own strengths and weaknesses. Consider trying a different algorithm if the current one is not performing well.\n","\n","\u003c/div\u003e\n"]},{"cell_type":"markdown","id":"487c94f2-1bc9-4db6-b547-7ec3c5d32371","metadata":{},"outputs":[],"source":["## Practice: Adding More Data for Training\n","\n","We can load more data and train the model with extra data.\n","\n","Lets have a quick look to more data:\n"]},{"cell_type":"code","id":"ba78f2aa-c7cc-4c7a-b772-9b3f09e5e201","metadata":{},"outputs":[],"source":["# getting more data\ntickers = [\"GOOG\",\"IBM\",\"AAPL\",\"META\",\"AMZN\"]  #\"^GSPC\" S\u0026P500\ndf = yf.download(tickers = tickers ,       # list of tickers\n                  period = \"2y\",         # time period\n                  interval = \"1h\",       # trading interval\n                  ignore_tz = True,      # ignore timezone when aligning data from different exchanges?\n                  prepost = False)       # download pre/post market hours data?  \nprint(\"shape of dataset: \", df.shape)\ndf.head()"]},{"cell_type":"markdown","id":"cc6246a9-fefd-4957-8546-d596b848b648","metadata":{},"outputs":[],"source":["### plotting the Adj Close for the 5 companies\n","\n","In the code below, loop through each stock in the 'Adj Close' column and plot.\n"]},{"cell_type":"code","id":"f8712d3a-66ce-4d0a-b7e7-0381aab7dc58","metadata":{},"outputs":[],"source":["import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsplit_line = int(np.round(len(df) * 0.1))  # use 10% of data for testing\n\n# Get min and max of 'Adj Close'\nadj_close_min = df['Adj Close'].min().min()\nadj_close_max = df['Adj Close'].max().max()\n\n# Set the background color\nsns.set_style(rc={'axes.facecolor': 'lightsteelblue'})\nplt.figure(figsize=(12, 4))\n\n# Loop through each stock in the 'Adj Close' column and plot\n# write your code here\n\n\nplt.xticks(rotation=45)\nplt.title(', '.join(df['Adj Close'].columns))\n\n# single vline with full ymin and ymax\nplt.vlines(x=df.index[-split_line], ymin=adj_close_min, ymax=adj_close_max, colors='green', ls=':', lw=2, label='Train-Test Split Point')\n\nplt.legend(bbox_to_anchor=(1.0, 1), loc='upper left')\nplt.show()\n"]},{"cell_type":"markdown","id":"aba2780a-758c-4b08-b984-2aec24b51f9b","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\n","    \u003csummary\u003eClick here for Solution\u003c/summary\u003e\n","\n","```python\n","for stock in df['Adj Close'].columns:\n","    sns.lineplot(data=df['Adj Close'][stock], label=f'Adjusted Close Price - {stock}')\n","```\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"b4a871f2-7674-4bb6-9d99-f0f7f2729356","metadata":{},"outputs":[],"source":["### Let's create a function that adds all the indicators to the dataframe\n","\n","Write `add_indicators` function adds several technical indicators (we had before) to a given DataFrame, and remove `na` and remove columns `Open`, `High`, `Low`, `Close`, `Volume`.\n"]},{"cell_type":"code","id":"dd070b2e-96c9-431d-8e59-db4c59500888","metadata":{},"outputs":[],"source":["# write you function here\n\n"]},{"cell_type":"markdown","id":"6f490671-d6e4-48ef-9da5-eaa22fece162","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\n","    \u003csummary\u003eClick here for Solution\u003c/summary\u003e\n","\n","```python\n","def add_indicators(df):\n","\n","    df['EMA7'] = ta.trend.EMAIndicator(close= df['Adj Close'], window= 7, fillna= False).ema_indicator()\n","    df['EMA14'] = ta.trend.EMAIndicator(close= df['Adj Close'], window= 14, fillna= False).ema_indicator()\n","    df['EMA50'] = ta.trend.EMAIndicator(close= df['Adj Close'], window= 50, fillna= False).ema_indicator()\n","    df['EMA200'] = ta.trend.EMAIndicator(close= df['Adj Close'], window= 50, fillna= False).ema_indicator()\n","    df['MACD_line'] =ta.trend.MACD(close= df['Adj Close'], window_slow= 26, window_fast= 12, window_sign = 9, fillna = False).macd()\n","    df['MACD_signal'] =ta.trend.MACD(close= df['Adj Close'], window_slow= 26, window_fast= 12, window_sign = 9, fillna = False).macd_signal()\n","    df['MACD_diff'] =ta.trend.MACD(close= df['Adj Close'], window_slow= 26, window_fast= 12, window_sign = 9, fillna = False).macd_diff()\n","    df['RSI']=ta.momentum.RSIIndicator(close= df['Adj Close'], window= 14, fillna= False).rsi()\n","    df['OBV']= ta.volume.OnBalanceVolumeIndicator(close= df['Adj Close'], volume= df['Volume'], fillna = False).on_balance_volume()\n","    df['BBH']=ta.volatility.BollingerBands(df['Adj Close'], window = 20, window_dev = 2, fillna = False).bollinger_hband_indicator()\n","    df['BBL']=ta.volatility.BollingerBands(df['Adj Close'], window = 20, window_dev = 2, fillna = False).bollinger_lband_indicator()\n","    df=df.dropna()\n","    df = df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'],axis=1)\n","    \n","    return df\n","```\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"186ee322-4d4e-40b6-a0ac-2e2599bda2d7","metadata":{},"outputs":[],"source":["Now, create separate data frame for each stock data.\n"]},{"cell_type":"code","id":"762ea36d-9df9-4128-9f51-3649acf021ed","metadata":{},"outputs":[],"source":["# loading individual stock data for every stock and calling add_indicator\n\ndf_GOOG = yf.download(tickers = 'GOOG' ,period = \"2y\", interval = \"1h\",ignore_tz = True,prepost = False) \ndf_GOOG = add_indicators(df_GOOG)\n\ndf_IBM = yf.download(tickers = 'IBM' ,period = \"2y\", interval = \"1h\",ignore_tz = True,prepost = False) \ndf_IBM = add_indicators(df_IBM)\n\ndf_AAPL = yf.download(tickers = 'AAPL' ,period = \"2y\", interval = \"1h\",ignore_tz = True,prepost = False) \ndf_AAPL = add_indicators(df_AAPL)\n\ndf_META = yf.download(tickers = 'META' ,period = \"2y\", interval = \"1h\",ignore_tz = True,prepost = False) \ndf_META = add_indicators(df_META)\n\ndf_AMZN = yf.download(tickers = 'AMZN' ,period = \"2y\", interval = \"1h\",ignore_tz = True,prepost = False) \ndf_AMZN = add_indicators(df_AMZN)\n\ntrain_data1 = df_GOOG.iloc[:-split_line,:]\ntrain_data2 = df_IBM.iloc[:-split_line,:]\ntrain_data3 = df_AAPL.iloc[:-split_line,:]\ntrain_data4 = df_META.iloc[:-split_line,:]\ntrain_data5 = df_AMZN.iloc[:-split_line,:]\n\ntrain_df_list = [train_data1,train_data2,train_data3,train_data4,train_data5]"]},{"cell_type":"markdown","id":"eace6a0b-1307-4f4a-8b2b-7f1f613a82b6","metadata":{},"outputs":[],"source":["### Exercise: optimize the above code\n","You can optimize the code by using a loop and a list of stock tickers. In the optimized code, we store the stock tickers in a list called `tickers`. We then loop through the list, downloading the stock data and adding indicators for each ticker. Finally, we split the data and add it to the `train_df_list`. This reduces the amount of repetitive code and makes it easier to add or remove stocks by simply modifying the `tickers` list.\n"]},{"cell_type":"code","id":"fd9fc056-7b4c-4109-9af5-552c7e13dca9","metadata":{},"outputs":[],"source":["# write your code here\n\n"]},{"cell_type":"markdown","id":"a73bccc1-33d6-4d0d-888e-c6f02fabec69","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\n","    \u003csummary\u003eClick here for Solution\u003c/summary\u003e\n","    \n","```python\n","\n","import yfinance as yf\n","\n","tickers = ['GOOG', 'IBM', 'AAPL', 'META', 'AMZN']\n","train_df_list = []\n","\n","for ticker in tickers:\n","    df = yf.download(tickers=ticker, period=\"2y\", interval=\"1h\", ignore_tz=True, prepost=False)\n","    df = add_indicators(df)\n","    train_data = df.iloc[:-split_line, :]\n","    train_df_list.append(train_data)\n","```\n","    \n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"9ca29eb4-05fe-4464-8abc-efc9166ff313","metadata":{},"outputs":[],"source":["### Training the Model with More Data\n"]},{"cell_type":"code","id":"21bb321c-6e50-4a49-9bb9-8ccc5f862fe9","metadata":{},"outputs":[],"source":["'''\ntraining loop to handle multiple dataframes like df_1, df_2, df_3, you can create a list of dataframes and loop through them. \n'''\n# Load the stock dataframes\ndata_list = train_df_list\n\n# Create and train the agent\nagent = DQNAgent(state_size, env.action_space, memory_size=4000) # increasing the memory size for the agent \nepisodes = 100\nbatch_size = 32\n\nfor data in data_list:\n    env = StockTradingEnv(data)\n    state_size = env.observation_space.shape[0]\n    action_size = env.action_space.nvec.prod()\n\n    for e in tqdm(range(episodes)):\n        state = env.reset()\n        state = np.reshape(state, [1, state_size])\n        total_reward = 0\n\n        while True:\n            action = agent.act(state)\n            next_state, reward, done, _ = env.step(action)\n            total_reward += reward\n            next_state = np.reshape(next_state, [1, state_size])\n\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n\n            if done:\n                # print(f'Episode: {e + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2}')\n                break\n\n        if len(agent.memory) \u003e batch_size:\n            agent.replay(batch_size)\n"]},{"cell_type":"raw","id":"bb97a6a9-02df-46f7-8881-1b3a1ca100db","metadata":{},"outputs":[],"source":["Have a cup of coffe, it will take 15 minutes to train the model on 5 datasets.\n\n      )  (\n     (   ) )\n      ) ( (\n    _______)_\n .-'---------|  \n( C|/\\/\\/\\/\\/|\n '-./\\/\\/\\/\\/|\n   '_________'\n    '-------'\n"]},{"cell_type":"markdown","id":"164b77aa-329f-4066-be6f-5057a2c2b14c","metadata":{},"outputs":[],"source":["### Testing the Data\n","\n","let's split the Apple stock data using `split_line` , that we defined before, for testing purpose.\n"]},{"cell_type":"code","id":"aa8ff649-a0f6-4c2c-a5be-a2ab7b9428aa","metadata":{},"outputs":[],"source":["# using Apple data for testing\n# write your code here\n\n\n\n# Create the test environment\ntest_env = StockTradingEnv(test_data,initial_cash =10000)\n\n# Define the number of Monte Carlo runs\nmonte_carlo_runs = 200\nrewards = []\n\nfor run in tqdm(range(monte_carlo_runs)):\n    # Test the agent\n    state = test_env.reset()\n    state = np.reshape(state, [1, state_size])\n    total_reward = 0\n\n    while True:\n        action = agent.act(state)\n        next_state, reward, done, _ = test_env.step(action)\n        total_reward += reward\n        next_state = np.reshape(next_state, [1, state_size])\n        state = next_state\n\n        if done:\n            # print(f'Test Run: {run + 1}/{monte_carlo_runs}, Total Reward: {total_reward}')\n            rewards.append(total_reward)\n            break\n\n# Calculate and print the average reward\naverage_reward = np.mean(rewards)\nprint(f'Average Reward after {monte_carlo_runs} runs: {average_reward}')\n"]},{"cell_type":"markdown","id":"5311e09f-8bbf-4914-8470-af7c64dfbf34","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\n","    \u003csummary\u003eClick here for Solution\u003c/summary\u003e\n","    \n","```python  \n","\n","test_data = df_AAPL.iloc[-split_line:,:]\n","\n","```\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"bddf6bbe-c163-4bdd-8019-2490c9a6d058","metadata":{},"outputs":[],"source":[" \n"," ### 💡 Some tips to increase the performance. \n"," Here are a few modifications you can make to improve the performance of your model: \n","- Normalize the input data: Scaling the input features can help your neural network to learn more effectively. Add a function in your `StockTradingEnv` class to normalize the input data, and apply it to your observation data:\n","\n","```python\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","class StockTradingEnv(gym.Env):\n","    # ...\n","\n","    def _normalize_data(self, data):\n","        scaler = MinMaxScaler()\n","        return scaler.fit_transform(data)\n","\n","    def _get_obs(self):\n","        # Calculate the observation vector for the current step\n","        obs = np.array([\n","            # ...\n","        ])\n","\n","        # Normalize the observation vector\n","        obs = self._normalize_data(obs)\n","\n","        # Return the observation vector\n","        return obs\n","```\n","\n"," \n","- Add more layers or increase the number of neurons in the neural network: A more complex model can potentially capture more intricate patterns in the data. However, keep in mind that increasing the complexity of the model might also increase the risk of overfitting. Modify the `_build_model` method in your `DQNAgent` class:\n","\n","```python\n","\n","def _build_model(self):\n","    # Build the neural network model\n","    model = Sequential()\n","    model.add(Dense(128, input_dim=self.state_size, activation='relu'))\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Dense(self.action_size, activation='linear'))\n","    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","    return model\n","```\n","\n"," \n","- Increase the memory size: A larger memory size can potentially help your agent remember more experiences and learn from them. Update the memory size in your `DQNAgent` class:\n","\n","```python\n","\n","class DQNAgent:\n","    def __init__(self, state_size, action_space):\n","        # ...\n","        self.memory = deque(maxlen=5000)\n","        # ...\n","```\n","\n"," \n","- Adjust hyperparameters: Experiment with different values for `gamma`, `epsilon_decay`, and `learning_rate` in your `DQNAgent` class. There is no one-size-fits-all solution for these hyperparameters, and you may need to try different combinations to find the best values for your specific problem.\n","\n","Remember that even with these modifications, it's important to test and validate the model on different datasets and time periods to ensure it generalizes well to unseen data. Also, note that the stock market is inherently unpredictable, and even a well-trained model may not always produce profitable results.\n"]},{"cell_type":"markdown","id":"1335e90e-5361-4f12-8fa0-f1cd8faf0fa6","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"5ba8dd70-9655-4b22-b3ef-a48cdaffff23","metadata":{},"outputs":[],"source":["|  |  |\n","|---|---|\n","| **Sina Nazeri**  | [linkedin](https://www.linkedin.com/in/sina-nazeri) |\n","| ![Sina](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0Q8REN/images/sina_low_qual.jpeg) | As a data scientist in IBM, I have always been passionate about sharing my knowledge and helping others learn about the field. I believe that everyone should have the opportunity to learn about data science, regardless of their background or experience level. This belief has inspired me to become a learning content provider, creating and sharing educational materials that are accessible and engaging for everyone. |\n"]},{"cell_type":"markdown","id":"e4948f56-94e9-433e-a7f2-c1fe875352ba","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"8c22c75f-d9b1-4d69-a3ad-642b42f0f115","metadata":{},"outputs":[],"source":["Joseph Santarcangelo\n","\n","J.C.(Junxing) Chen \n","\n","Sheng-Kai Chen \n","\n","Artem Arutyunov \n","\n","Justin Correia \n","\n","Roodra Kanwar \n","\n","Vicky Kuo\n"]},{"cell_type":"markdown","id":"7c144cdf-c934-4a70-ae65-32c0a98f8f2b","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"e5679588-0a96-4ecb-9844-e46843d822f4","metadata":{},"outputs":[],"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2023-06-01|0.1|Sina Nazeri|Create Lab Template|\n"]},{"cell_type":"markdown","id":"166935f5-b40b-4e1a-a64f-1237535b9283","metadata":{},"outputs":[],"source":["Copyright © 2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}